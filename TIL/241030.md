# 241029 20일차 TIL

#1030 문제 해결 !
#진행을 못하는동안 왜그렇게 예측값이 낮았는지 곰곰히 생각을 해봤는데 아무리 생각해도, 건축년도를 집어 넣은게 예측값에 크게 영향을 주는것 같다는 생각을했다.
#방 갯수, 화장실 갯수 주차차량,토지면적 등은 주택가격에 영향을 주기 쉽지만
#건축년도 같은경우는 1800년대에 지었지만 오히려 가치가 높은 경우도 있을수 있으니까 ?
#그래서 특징을 오히려 빼주면 예측치가 올라갈거 같다는 희망을 가진상태에서 바로 진행했지만 
#Mse 값은 400억? R-squared Score:0.08382252636820098
#랜덤포레스트 같은경우 0.005 오히려 더 낮은 값을 보여준다.
#이후 데이터가 너무 적어서 그럴수 있을거같아서 이상치를 제거하는게 아닌 최빈값으로 바꿔주었지만
#Mean Squared Error: 49956150586.94633 R-squared Score: 0.08722231836268579 랜덤포레스트 Accuracy: 0.036690433749257276
#아무리 생각해도 진행이 혼자 어려워서 튜터님께 문의 드렸지만 퇴근이슈로
#코드랑 데이터 튜터님께 전달하고 내일 같이 해결예정
이후 머신러닝 모델 개념을 복습하면서 확인한 부분인데 내가 진행한 로지스틱 회귀 같은경우는
이진형일때 사용되는 기법이었는데 그걸 가지고 가격 예측을 하려고 했었다.
그러니 당연히 예측치 값이 널뛰기 했던것이다.


> ###### 이후 머신러닝 개념에 대해 다시한번 복습



> 지도학습 : 회귀모델
> 선형회귀

종속 변수와 하나 이상의 독립 변수 간의 선형관계를 모델링하는 방법이다.

독립변수의 수에 따라 단순 선형회귀와 다중 선형회귀로 나뉜다.

 

> 학습이란?

머신러닝에는 가중치라는것이 잇고 가중치에 따라 예측값이 결정됨 예측치와 실제값의 차이의 오차를 계산할수있고,

이 오차를 최소화할수있는 법은 미분이있다. 이 미분을 통해 가중치를 바꿔가며 오차가 줄어들게 하는것이다 

오차가 줄어드는 이 과정을 학습이라한다.


> train_test_split 

train_test_split 함수는 학습용 데이터셋과 테스트용 데이터셋을 반환해주는 함수이며 ,

반환값은 순서대로 학습용 데이터셋, 테스트용 데이터셋, 학습용 레이블, 테스트용 레이블이다.

X_train =학습용 데이터셋 X_train =  테스트용 데이터셋  y_train= 학습용레이블 , y_test = 테스트용 레이블

선형 회귀 모델을 생성하고 학습시키기 위해 

model = LinearRegression() 넣어주면 선형모델을 자동으로 생성해주며

model.fit(X_traion,y_train) 에 테스트 데이터와 테스트 정답을 순서로 넣어주면 자동으로 학습을 시작한다.

이후 예측을 확인하기위해 y_pred라는 변수를 설정하고 테스트할 독립 변수를 넣어 예측값을 확인할수있다.




> 다항회귀

비선형 모델 관계를 모델링하는 방법 이며 , 독립변수의 다항식을 사용하여 관계를 모델링한다.

좀더 쉽게 생각하자면 , 데이터 각 특성에 제곱을 추가하여 선형 회귀 모델을 훈련시키는 방법이라고 생각하면 될거같다.

기존 데이터의 특성을 제곱하는 방법으로는 sklearn 의 PolynomialFeatures 함수를 이용 할수있고,

degree를 통해 계수를 선택할수있다. 현재는 2를 넣어 2차항으로 진행하였다.

이후 poly.fit_transform을 통해 실제로 2차항으로 만들어준다.


> 로지스틱 회귀 - 분류모델



종족 변수가 이진형일때 (즉, 결과가 두가지중 하나일때) 사용되는 통계 기법.

로지스틱 회귀는 선형회귀와 달리 결과값이 0과 1사이에 위치하게 하기 위해 시그모이드 함수를 사용한다.

로지스틱 회귀는 데이터를 학습하여 각 데이터 포인트가 특정 클래스에 속할 확률을 예측한다.

> 분류모델 - SVM

분류와 회귀분석에 사용되는 강력한 모델.

데이터를 분류하기 위해 결정경계를 찾아 분류함.

초평면은 두 클래스 사이의 최대 마진을 보장하는 방식으로 선택함.

> 분류모델 - KNN

KNN 알고리즘은 분류와 회귀 분석에 사용되는 비모수적 방법

새로운 데이터 포인트를 기존 데이터 포인트 중 가장 가까운 K개의 이웃과 비교하여 분류

데이터 포인트의 특성을 기준으로 거리 계산을 통해 가장 가까운 이웃을 찾는다

> 분류모델 - 나이브베이즈


베이즈 정리를 기반으로 하는 통계적 분류기법 나이브라는 이름이 붙은 이유는 각 특징이 독립적이라고 가정하기 때문이다.
주로 텍스트 분류 문제에서 널리 사용한다. 나이브베이즈의 종류

* 가우시안 나이브 베이즈 : 특징들이 연속적이고 정규 분포를 따른다고 가정.

* 베르누이 나이브베이즈 : 특징들이 이진수(0,1)로 표현되는 경우 사용.

* 멀티노미얼 나이브베이즈 : 특징들이 다항 분포를 따르는 경우 사용.

> 분류모델 - 의사결정나무

예측모델 중 하나로 , 데이터의 특징을 기준으로 의사결정 규칙을 만들고 이를 바탕으로 데이터를 분류하거나 회귀하는데 사용한다.

의사결정나무는 트리 구조를 가지며 , 각 내부 노드는 데이터의 특정 특징에 대한 테스트를 나타내고 ,

각 가지는 테스트 결과를 나타내며, 각 리프노드는 클래스 레이블을 나타낸다.

