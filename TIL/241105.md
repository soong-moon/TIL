# 241105 24일차 TIL

>  ##### Transformers 라이브러리란?

`Transformers` 라이브러리는 다양한 NLP 모델을 쉽게 사용할 수 있도록 도와주는 Hugging Face의 오픈소스 라이브러리.
이 라이브러리를 통해 최신 NLP 모델들을 불러와 텍스트 생성, 감정 분석, 번역 등 다양한 작업에 활용할 수 있다.


> ##### 다양한 NLP 모델 살펴보기

1. GPT-2 
OpenAI 에서 개발한 언어 생성 모델로, 문장을 생성하거나 이어지는 텍스트를 예측하는 데 뛰어난 성능을 발휘한다.
GTP-2 Transformer 라이브러리에서 바로 사용 가능 하며, 여러 텍스트 생성 작업에 활용할수 있다.

```
from transformers import pipeline

# GPT-2 기반 텍스트 생성 파이프라인 로드
generator = pipeline("text-generation", model="gpt2")

# 텍스트 생성
result = generator("Once upon a time", max_length=50, num_return_sequences=1)
print(result)
```
GTP-2 예시 . 파이프라인에는 작업을 지시할수 있고 모델을 지정할수 있다.
기본 모델들이 설정되어 있는 경우가 있어서 모델을 지정하지 않아도 작업은 진행할수있다.

2. 간단한 감성어 분석
이 파이프라인은 텍스트 데이터를 입력으로 받아 그 텍스트의 감정을 분석하는 작업을 수행한다.
감정 분석은 주로 텍스트가 긍정적인지,부정적인지, 또는 중립적인지 판단하는데 사용된다.
```
from transformers import pipeline

# 감정 분석 파이프라인 로드
sentiment_analysis = pipeline("sentiment-analysis")
result = sentiment_analysis("I love using Hugging Face!")
print(result)
``` 
간단한 감성어 분석 예시 실행했을시 POSITIVE가 나오게된다.그리고 score로 어느정도 그 감정에 치우쳐져 있는지도 확인가능하다.

3. RoBERTa
RoBERTa는 BERT 모델을 최적화한 버전으로, 더 많은 데이터와 더 긴 학습 시간을 통해 성능을 향상시켰다.
RoBERTa는 텍스트 분류와 감정 분석 같은 작업에서 뛰어난 성능을 자랑한다.
```
from transformers import pipeline

# RoBERTa 기반 감정 분석 파이프라인 로드
classifier = pipeline("sentiment-analysis", model="roberta-base")

# 감정 분석 실행
result = classifier("This product is amazing!")
print(result)  # [{'label': 'POSITIVE', 'score': 0.9998}]
```
감정 분석 예시인데 결과값이 위와 다르게 `[{'label': 'LABEL_0', 'score': 0.5337594747543335}]` 이상하게 나온다.
이유는 BERT모델에 있는데, BERT기반 모델은 핵심적인 내용은 대규모 데이터셋을 통해 사전 학습시켜놓고 마지막 레이어,
즉 번역이나 분류등 목적에 따라 바꿀 필요가 있는 레이어는 학습자체를 사용자에게 맡기는 경우가 있기 때문이다.
(특정 작업에 바로 사용할수 있는 가중치가 없기 때문에 **파인튜닝**이 필요하다.)

> ##### 임베딩 개념
임베딩은 고차원 데이터를 저차원 공간으로 변환하는 기법.
주로 벡터표현으로 구현되는데, 숫자의 나열로 데이터의 의미는 유지하면서 비교적 작은 차원의 공간에서 데이터를 표현하는것을 말한다.
Ai모델은 숫자를 잘 다루는데 , 자연어처리나 이미지 처리 등 에서는 비정형 데이터가 주로 다루어지기때문에
이러한 비정형 데이터를 모델이 잘 처리하는 수치화 데이터로 변환해줄 필요가 있고, 이럴때 임베딩을 사용한다고 생각하면 된다.

> ##### 임베딩의 종류

1. 워드임베딩 - 자연어처리에서 단어를 벡터로 표현해준다. 
2. sentence임베딩 - 문장 전체를 벡터로 표현해준다.
3. 이미지 임베딩 - 이미지 데이터를 벡터로 표현해준다.
4. 그래프 임베딩 - 그래프 구조를 벡터로 표현해준다.

> ##### 사전 학습이란 ?
대규모의 텍스트 데이터셋을 사용해 모델이 일반적인 언어 이해 능력을 학습하는 과정.
이단계에서는 특정작업(번역,감정분석)을 염두에 두지 않고, 단순히 언어의 패턴과 구조를 학습하는것이 목적이다.

> ###### 사전 학습의 특징
1. 대규모 데이터셋 사용
인터넷에서 수집한 방대한 양의 텍스트 데이터로 모델을 학습시킨다.
2. 일반적인 언어 이해
모델은 텍스트 내 단어의 의미,문장 구조, 문맥 등 언어의 전반적인 특징을 학습한다.
3. 작업 비특화
특정 작업에 맞춰진 학습이 아닌, 전반적인 언어 이해에 초점을 맞춘다.



> ##### 파인 튜닝이란?
사전 학습된 모델을 특정 작업에 맞게 추가로 학습시키는 과정이다.
예를 들어 ,BERT 모델을 감정 분석에 사용하려면, BERT의 사전 학습된 가중치를 유지하면서 감정 분석 작업에 맞게 모델의 가중치를 조절한다.

> ###### 파인 튜닝의 특징
1. 작업 특화 
파인 튜닝은 특정작업 (텍스트 분류, 번역 , 질의 응답 등)에 맞춰 모델을 최적화하는 과정이다.
2. 사전 학습 가중치 활용
사전 학습된 모델의 언어 이해 능력을 바탕으로, 새로운 작업에 적응할 수 있도록 일부 가중치만 조정된다.
3. 적은 데이터로도 가능
사전 학습 덕분에 , 파인 튜닝은 비교적 적은 양의 데이터로도 효과적인 학습이 가능하다.

>###### IMDb 데이터셋을 활용한 BERT 파인 튜닝 실습

```
dataset = load_dataset('imdb')

test_dataset = dataset('test').shuffle(seed=42).select(range=500)

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
def tokenize_function(examples):
    return tokenizer(examples['text'], padding="max_length", truncation=True)
#examples['text'] = imdb 에서 리뷰부분을 가져오는것     
#padding="max_length" = 모든 텍스트를 최대길이로 패딩  
#truncation=True = 텍스트의 길이가 최대 길이를 초과하면 잘라내는것

test_dataset = test_dataset.map(tokenize_function, batched=True)
#데이터셋에 각 샘플에 tokenize_function 적용하는과정
#batch 단위로 함수를 적용하는거라 여러함수를 적용시킨다
test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])
#데이터셋을 특정 형식으로 변환 이 예시에서는 torch 로 변환한다
#columns에는 BERT모델에 필요한 컬럼만 넣는다.     
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
```
